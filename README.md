# AI Content Moderator - Hinge PM Interview Project

## Project Overview
Building a hands-on AI content moderation system to demonstrate technical skills for the **AI Product Manager, Trust & Safety** role at Hinge.

**Current Learning Mode:** Deep understanding approach - explaining technical choices and reasoning for interview preparation.

## Learning Objectives
Develop interview-ready expertise in key job requirements:

### Core Technical Skills
- [x] **Chain of Thought prompting** - 5-step dating app specific reasoning process
- [x] **Problem Analysis** - Identified dating app specific moderation challenges
- [x] **Langfuse integration** - LLM tracing and performance analysis with automatic OpenAI monitoring
- [x] **Evaluation Framework Design** - Ground truth dataset with violation severity scoring
- [x] **False positive/negative analysis** - Systematic prompt evaluation infrastructure
- [x] **Python prototyping** - Comfortable modifying AI workflows
- [ ] **A/B testing framework** - Comparing prompt strategies with measurable outcomes

### Interview Story Arc
*"Leadership asked me to prototype AI moderation for the queue. I analyzed our pain points - volume, speed, and context complexity - then built Chain of Thought prompts that handle dating app consent and context nuances..."*

## Project Structure
```
/moderation/
‚îú‚îÄ‚îÄ main.py                   # Original prototype (reference only)
‚îú‚îÄ‚îÄ hinge_moderation_v2.py    # New prototype built from scratch with full understanding
‚îú‚îÄ‚îÄ .env                      # API keys (secure, not committed)
‚îú‚îÄ‚îÄ requirements.txt          # Dependencies
‚îú‚îÄ‚îÄ .gitignore               # Protects API keys
‚îî‚îÄ‚îÄ README.md                # This file
```

## Cross-Platform Setup

### PC (WSL2) Setup
```bash
cd /home/matt/projects/moderation
source .venv/bin/activate
pip install openai python-dotenv
python main.py
```

### MacBook Setup
```bash
cd ~/Documents/Github/moderation-sep-25/moderation
python3 -m venv .venv
source .venv/bin/activate
pip install openai python-dotenv
echo "OPENAI_API_KEY=your_key_here" > .env
python3 main.py
```

## Current Status
‚úÖ **Environment Setup** - Cross-platform Python venv, OpenAI API integration
‚úÖ **PRD Development** - Business problem, success metrics, technical strategy defined
‚úÖ **5-Step Chain of Thought Design** - Complete AI reasoning framework designed
‚úÖ **Prototype Foundation** - Basic Python structure built and tested (`hinge_moderation_v2.py`)
‚úÖ **Interview-Ready Logic** - Can defend technical choices and business outcomes
‚úÖ **Complete Text Moderation** - GPT-4 integration with Chain of Thought reasoning
‚úÖ **Real Policy Integration** - Actual Hinge community guidelines loaded and applied
‚úÖ **Multi-Modal AI System** - GPT-4V image moderation with visual content analysis
‚úÖ **Professional Web Interface** - Streamlit demo with dual-mode display
‚úÖ **Production-Grade Error Handling** - AI safety system integration and graceful degradation
‚úÖ **Langfuse Observability** - Full LLM tracing with automatic monitoring and cost tracking
‚úÖ **Evaluation Framework** - Ground truth dataset with systematic prompt performance testing
‚úÖ **Live Demo Ready** - http://172.20.152.153:8502 for Monday presentation
üéØ **ENTERPRISE-GRADE SYSTEM** - Production AI moderation with observability and evaluation

## Current Context - Strategic PM Learning Phase
**Where we are:** Building complete PM workflow from business problem to technical solution

**Role-Play Scenario:**
- **Role:** Trust & Safety PM at Hinge (AI background from Maven courses)
- **Challenge:** Leadership wants path to 100% AI moderation + agent efficiency
- **Timeline:** Thursday evening ‚Üí Monday morning presentation
- **Constraints:** Focus on text/images first, video later
- **üö® CRITICAL:** Must have working prototype ready for live demo Monday

**Mock Data Analysis in Progress:**
- Text messages: 45K daily reports (78% volume), 12% appeal rate
- Photos: 12.5K daily reports (22% volume), 23% appeal rate
- Geographic: English 60% (11% appeals), Spanish 25% (19% appeals)
- Harm priorities: Financial scams (high harm), NCII (highest harm), harassment (medium harm)

**Learning Goals Achieved:**
1. ‚úÖ Strategic thinking - Data-driven prioritization approach
2. üîÑ Technical evaluation - Why Chain of Thought, why Langfuse
3. ‚è≥ Execution planning - Phases, metrics, risks

## Tomorrow's Critical Path (Friday, Sep 26)

### Morning Session: Strategic Analysis ‚úÖ COMPLETED
- [x] **PRD Development:** Business problem, success metrics, technical strategy
- [x] **Chain of Thought Design:** Complete 5-step AI reasoning framework
- [x] **Prototype Foundation:** Built `hinge_moderation_v2.py` from scratch with full understanding

### Afternoon Session: Implementation
- [ ] **Code Chain of Thought:** Implement 5-step reasoning into prototype
- [ ] **OpenAI Integration:** Connect AI logic to GPT-4 API
- [ ] **Demo Scenarios:** Test with realistic dating app content

### Evening Session: Presentation Prep
- [ ] **Demo Run-Through:** Test prototype with realistic scenarios
- [ ] **Edge Cases:** Show AI reasoning and human escalation triggers
- [ ] **Failure Scenarios:** Demonstrate when human review is needed

## Monday Presentation Structure
**Part 1:** Strategic Recommendation (Data ‚Üí Problem ‚Üí Solution)
**Part 2:** Live Prototype Demo (Proof of concept validation)
**Part 3:** Implementation Roadmap (Timeline, resources, risks)

## Interview Story Arc Complete
*"Leadership challenged me to solve our moderation bottleneck. I analyzed our data, identified the highest-impact opportunities, built a Chain of Thought prototype with Langfuse observability, and demonstrated measurable improvements in decision accuracy and speed."*

## Key Job Requirements Mapping

| Hinge JD Requirement | Our Implementation | Interview Story |
|---------------------|-------------------|----------------|
| "Chain of Thought reasoning" | 5-step moderation framework with real Hinge policies | "I engineered a systematic reasoning process that breaks down decisions..." |
| "LLM tracing platforms (Langfuse)" | ‚úÖ **Live integration** with automatic monitoring | "I integrated Langfuse for real-time observability - here's my dashboard..." |
| "False-positive/negative rates" | Tracing infrastructure ready for error analysis | "I built the foundation to track and analyze moderation accuracy..." |
| "Python for prototyping" | ‚úÖ **Complete system** built from scratch | "I prototyped and iterated on multi-modal AI workflows..." |
| "A/B tests" | Langfuse infrastructure enables prompt comparison | "I can now A/B test different prompt strategies with measurable data..." |

## Interview Questions to Practice
*Will be added after each major section completion*

## Notes for Future Sessions
- Focus on **understanding** over building - need to explain technical choices
- Each section ends with 3 interview questions to test comprehension
- Goal: Speak confidently about every bullet point in the job description

---

## Key Files for Tomorrow
- `main.py` - Current Chain of Thought prototype (needs demo enhancement)
- `README.md` - Project documentation and progress tracking
- Mock data metrics from today's session (in README)

## Tomorrow's Success Criteria
‚úÖ **Strategic clarity:** Can articulate data-driven prioritization
‚úÖ **Technical demonstration:** Working prototype with Langfuse tracing
‚úÖ **Business presentation:** Ready for Monday leadership demo

---
## Morning Session Accomplishments (Sep 26, 9:24am - 12pm)

### ‚úÖ PRD & Business Strategy
- **Business Problem Defined:** 57.5K daily reports, 12-23% appeal rates, $5M annual cost
- **Success Metrics:** 10% appeal reduction, 95% AI confidence, 25-50% faster decisions
- **Technical Strategy:** Chain of Thought + Langfuse for transparency and continuous improvement

### ‚úÖ 5-Step Chain of Thought Framework Designed
1. **Context Analysis:** Content type, engagement signals, conversation context
2. **Intent Assessment:** Harm scoring, account metadata, tiered response system
3. **Policy Application:** Severity-ordered violations, emergency escalation for CP/suicide
4. **Evidence Documentation:** NYT publishability standard, bias checking
5. **Action Recommendation:** Self-correction, user-centric outcomes, auto-ban triggers

### ‚úÖ Prototype Built from Scratch
- **File:** `hinge_moderation_v2.py` - Complete understanding of every component
- **Structure:** Python classes, imports, Chain of Thought placeholder
- **Testing:** Basic structure runs successfully
- **Learning:** Syntax debugging, virtual environments, development workflow

### üîÑ Current Status: Ready for Implementation
- Chain of Thought logic designed and interview-ready
- Technical foundation built with full understanding
- Ready to code AI reasoning into working prototype

---

## Friday Afternoon Accomplishments (Sep 26, 2:00pm - 6:00pm)

### ‚úÖ Complete Multi-Modal AI Moderation System
- **Text Moderation**: GPT-4 with 5-step Chain of Thought reasoning
- **Image Moderation**: GPT-4V visual content analysis with same framework
- **Policy Integration**: Real Hinge community guidelines (`hinge-principles.txt`)
- **Error Handling**: Production-grade safety system integration

### ‚úÖ Professional Web Interface (`web_demo.py`)
- **Dual-Mode Display**: Demo mode (full AI reasoning) vs User mode (clean output)
- **Multi-Modal Support**: Single interface for text and image uploads
- **Live Demo URL**: http://172.20.152.153:8502
- **Interactive Testing**: Real-time content analysis for recruiters

### ‚úÖ Advanced AI Product Insights Discovered
- **AI Safety Limitations**: GPT-4V refuses explicit content analysis
- **Non-Determinism**: Same content produces different scores (3 vs 5)
- **Production Challenges**: Need for human-in-the-loop systems
- **LangFuse Integration**: Clear value proposition for observability

### ‚úÖ Interview-Ready Demo Assets
**Technical Demonstration**:
- Multi-modal AI system with consistent reasoning framework
- Real policy integration showing business application
- Error handling demonstrating production thinking
- Debug capabilities showing technical depth

**PM Story Arc**:
*"Built Chain of Thought moderation, extended to images, discovered AI safety limitations through testing, and created dual interfaces for different stakeholders. Ready to present scalable solution with clear next steps."*

### üéØ Monday Presentation Strategy
1. **Live Demo**: Interactive web interface with multiple content types
2. **Technical Depth**: Show both user experience and AI reasoning
3. **Business Value**: Real Hinge policies with measurable outcomes
4. **Advanced Insights**: AI limitations and production considerations
5. **Scalability**: Clear path to 100% AI moderation with human oversight

## Production Observability with Langfuse Integration

### What We Built (Sep 26, Evening)
- **Automatic LLM Tracing**: Every OpenAI API call automatically tracked and analyzed
- **Real-Time Dashboard**: Live monitoring of prompts, responses, tokens, costs, and latency
- **Zero-Code Integration**: Simple import change enables comprehensive observability
- **Production Insights**: Track false positives, prompt performance, and system reliability

### Technical Implementation
```python
# Before: Basic OpenAI integration
from openai import OpenAI
client = OpenAI(api_key=api_key)

# After: Langfuse-wrapped OpenAI with automatic tracing
from langfuse.openai import openai
client = openai.OpenAI(api_key=api_key)
```

### Environment Configuration
```
OPENAI_API_KEY=sk-proj-...
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_HOST=https://us.cloud.langfuse.com
```

### Dashboard Capabilities Demonstrated
- **Trace Analysis**: Complete prompt-to-response journey for each moderation decision
- **Performance Metrics**: Token usage, API latency, and cost tracking per request
- **Error Monitoring**: Automatic capture of API failures and safety filter triggers
- **Session Grouping**: Organize traces by content type (text vs image moderation)

### Interview Value Proposition
*"I integrated Langfuse observability to monitor our AI moderation pipeline in real-time. This gives us production-grade insights into prompt performance, cost optimization opportunities, and enables data-driven false positive analysis - exactly what you need for scaling AI moderation at Hinge."*

### Advanced Features Ready for Implementation
- **Custom Metadata**: Tag traces with content_type, user_id, moderation_outcome
- **A/B Testing**: Compare different prompt strategies with measurable performance data
- **Alert Systems**: Monitor for unusual patterns or performance degradation
- **Cost Optimization**: Track token usage patterns to optimize prompt efficiency

## Systematic Prompt Evaluation Framework

### What We Built (Sep 26, Evening Session 2)
- **Ground Truth Dataset**: Comprehensive test cases with expected scores and actions
- **Violation Severity Scale**: 1-10 scoring system with clear action thresholds
- **Realistic Test Cases**: Actual dating app content patterns including emojis and innuendo
- **Advanced Evaluation Logic**: Accounts for cultural context, violation stacking, and behavioral patterns

### Evaluation Scale Design
```
Score 1-3: Harmless but trackable (cheesy pickup lines)
Score 4-5: Cultural/borderline content (movie quotes with innuendo)
Score 5-6: Clear boundary violations (inappropriate first date suggestions)
Score 7-8: Harassment requiring removal
Score 9-10: Immediate safety threats requiring instant ban
```

### Key Evaluation Principles Discovered
- **Literal vs. Implied Meaning**: Context matters more than literal interpretation
- **Violation Stacking**: Multiple violations in one message = exponential severity increase
- **Cultural Context Awareness**: Movie quotes require different treatment than original content
- **Behavioral Pattern Recognition**: Track all content to identify escalating users

### Test Dataset Examples
```json
{
  "content": "I don't bite, you know‚Ä¶ unless it's called for",
  "expected_score_range": [4, 5],
  "expected_action": "approve_with_tracking",
  "violation_type": "borderline_aggressive",
  "reasoning": "Sexual innuendo with mild aggression theme - track for escalation patterns"
}
```

### Interview Value Proposition
*"I built a systematic evaluation framework that balances user freedom with safety - distinguishing between cultural references and original harassment while tracking behavioral patterns to prevent escalation."*

### Production Applications
- **Prompt Optimization**: Measure current prompt performance against ground truth
- **A/B Testing Infrastructure**: Compare different reasoning strategies with concrete metrics
- **False Positive Reduction**: Systematic approach to improve user experience
- **Escalation Prevention**: Early detection of problematic user patterns

---

## Thursday Evening Session (Sep 26, 2025) - AI Evaluation Deep Dive

### ‚úÖ Evaluation Framework Implementation
- **Working Evaluation System**: Successfully built `run_evaluation()` function that tests all 20 ground truth cases
- **Score Parsing Logic**: Added `parse_score_from_response()` with regex extraction of 1-10 scores
- **Accuracy Infrastructure**: Built foundation to compare AI scores against expected ranges
- **Langfuse Integration**: All evaluation runs automatically traced for production monitoring

### üîç Critical AI PM Discovery: The Observability Crisis
**Problem Identified**: Content being analyzed is buried in verbose prompts (2000+ characters of guidelines), making debugging impossible in Langfuse traces.

**Key Learning**: *"Demo systems can hide complexity, but production requires observability-first design"*

**Real-world Impact**:
- False positive analysis becomes impossible
- Team can't debug edge cases
- User complaints can't be traced to root cause
- A/B testing prompts becomes ineffective

**Everyday Analogy**: *"Like a restaurant receipt where the actual order is buried at the bottom of 3 pages of legal text - management can't quickly see what was ordered when there's a complaint."*

### üéØ Strategic Pivot: Evaluations Before Accuracy
**Critical Insight**: Discovered need for **consistency testing before accuracy measurement**
- Same input might produce different scores (non-determinism)
- Must verify prompt reliability before measuring against ground truth
- **Core AI PM Principle**: "Fix consistency first, then measure accuracy"

### üöÄ Technical Achievements
- **Web Demo Active**: Streamlit interface running at http://localhost:8501
- **Complete Evaluation Pipeline**: Dataset ‚Üí AI Analysis ‚Üí Score Extraction ‚Üí Accuracy Calculation
- **Production Observability**: Every moderation decision automatically tracked in Langfuse
- **Interview-Ready Architecture**: Demonstrates systematic AI evaluation methodology

### üìã Tomorrow's Critical Path (Friday AM on PC)
1. **Consistency Testing**: Build `test_consistency()` to verify prompt reliability
2. **Observability Fix**: Separate content from guidelines for better trace debugging
3. **Accuracy Analysis**: Complete evaluation report with false positive detection
4. **A/B Testing Setup**: Compare different prompt strategies with measurable data

### üé§ New Interview Story Elements
*"I built systematic evaluation infrastructure and discovered a critical observability gap - content was buried in verbose prompts, making production debugging impossible. This taught me that demo systems can hide complexity, but production AI requires observability-first architecture. I pivoted to consistency testing before accuracy measurement, demonstrating systematic AI evaluation methodology."*

### Key Job Requirements Demonstrated
- **Systematic prompt evaluation** ‚úÖ Built complete infrastructure
- **False positive analysis** ‚úÖ Framework ready, discovered observability prerequisites
- **LLM tracing with Langfuse** ‚úÖ Production monitoring active
- **A/B testing foundation** ‚úÖ Infrastructure ready for prompt comparison

## RAG Optimization for Production (Future Implementation)

### Cost Inefficiency Identified
**Current Problem**: Each moderation decision sends entire Hinge principles (~1000+ tokens) to GPT-4
- 20 test cases = 20,000+ redundant policy tokens
- Scales poorly: 57.5K daily reports would cost $X in redundant policy transmission

### RAG Solution Architecture
```python
# Instead of: prompt = f"Apply guidelines: {self.principles}"  # 1000+ tokens
# Use RAG: relevant_policies = retrieve_relevant_policies(content, context)  # 100-200 tokens
```

**Implementation Plan:**
1. **Policy Embeddings** - Convert each Hinge rule to vector embeddings
2. **Content Analysis** - Embed user content being moderated
3. **Semantic Retrieval** - Find 2-3 most relevant policies per case
4. **Contextual Generation** - Send only relevant rules to GPT-4

**Expected Impact:**
- 70% token cost reduction
- Faster API responses (smaller prompts)
- Better observability (content not buried in verbose prompts)
- Maintained decision accuracy

**Interview Value:**
*"Discovered cost inefficiency in prototype, recognized RAG opportunity, redesigned for production scalability - demonstrates hands-on RAG experience from job requirements."*

## Systematic Evaluation with Langfuse Complete

### ‚úÖ Friday Morning Session Accomplishments (Sep 27, 2025)

**Evaluation Infrastructure Built:**
- ‚úÖ **Complete accuracy calculation** - False positive/negative analysis with severity tracking
- ‚úÖ **Langfuse integration** - 48 traces generated and human-annotated across 4 score dimensions
- ‚úÖ **Annotation queue system** - Systematic review with standardized scoring criteria
- ‚úÖ **Pattern identification** - Comprehensive tagging system for prompt optimization insights

**Score Configurations Implemented:**
1. **Moderation Accuracy** (categorical): accurate, false_positive, false_negative
2. **Content Severity** (1-10): Harmless ‚Üí Borderline ‚Üí Violations ‚Üí Serious threats
3. **Action Quality** (categorical): approve, approve_with_monitoring, warning, reject, immediate_ban
4. **Reasoning Quality** (1-5): Chain of Thought process evaluation

**Systematic Tagging Framework:**
- **False Positive Tags**: fp_compliment, fp_date_request, fp_phone_number_request, fp_severity_inflation
- **False Negative Tags**: fn_harassment_missed, fn_boundary_violation_missed, fn_pressure_tactics
- **Accurate Case Tags**: accurate_appropriate, accurate_violation, demo_ready

**Key Discovery - Cost Inefficiency:**
- Each moderation decision sends 1000+ tokens of Hinge principles
- 20 test cases = 20,000+ redundant policy transmission
- RAG optimization roadmap documented for 70% cost reduction

**Current Status:**
- **48 human-annotated traces** with comprehensive scoring
- **Pattern analysis ready** for targeted prompt optimization
- **Demo-critical insights** identified for Monday presentation
- **Production evaluation methodology** established

### Next Steps:
1. **Tag pattern analysis** - Identify most frequent failure modes
2. **Prompt optimization** - Target highest-impact improvements for demo reliability
3. **Consistency testing** - Validate prompt reliability across multiple runs

**Interview Value Proposition:**
*"Built comprehensive AI evaluation system using automated metrics, human annotation, and systematic pattern analysis in Langfuse. Discovered cost optimization opportunities and established production-grade evaluation methodology for scaling AI moderation."*

---
*Last updated: Sep 27, 2025 - Complete evaluation framework with human validation and optimization insights*